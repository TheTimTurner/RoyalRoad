{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import random\n",
    "#!pip install ProgressBar\n",
    "from progressbar import ProgressBar\n",
    "import math\n",
    "#!{sys.executable} -m pip install ProgressBar\n",
    "#!pip install ProgressBar\n",
    "from random import randrange\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "#svd stuff\n",
    "from scipy.linalg import svd \n",
    "from numpy.linalg import pinv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the titles and wheater the book is alive or dead into two seperate arrays\n",
    "def loadResult():\n",
    "    test_labels = []\n",
    "    test_values = []\n",
    "    with open('table-of-contents.json', encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "        for books in data:\n",
    "            for book in data[books]:\n",
    "                test_labels.append(book['title'])\n",
    "                test_values.append(book['dead'])\n",
    "                \n",
    "    return test_labels, test_values\n",
    "\n",
    "#get the data from the data output file\n",
    "def loadData():\n",
    "    final_data = []\n",
    "    with open('dataOutput2.json', encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "        for index in data: \n",
    "            final_data.append(data[index]['chapData'])\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16705\n",
      "Training dataset dimensions:  (15705, 71)\n",
      "Number of training labels:  15705\n",
      "Testing dataset dimensions:  (1000, 71)\n",
      "Number of testing labels:  1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                \n",
    "\n",
    "title_labels, dead_labels = loadResult()\n",
    "train = loadData()\n",
    "print(len(train))\n",
    "## Load the training set\n",
    "train_data= np.array(train[:15705])\n",
    "train_labels = np.array(dead_labels[:15705])\n",
    "train_titles = np.array(title_labels[:15705])\n",
    "\n",
    "\n",
    "## Load the testing set\n",
    "\n",
    "test_data= np.array(train[-1000:]) \n",
    "test_labels = np.array(dead_labels[-1000:])\n",
    "test_titles = np.array(title_labels[-1000:])\n",
    "\n",
    "\n",
    "## Print out their dimensions\n",
    "print(\"Training dataset dimensions: \", np.shape(train_data))\n",
    "print(\"Number of training labels: \", len(train_labels))\n",
    "print(\"Testing dataset dimensions: \", np.shape(test_data))\n",
    "print(\"Number of testing labels: \", len(test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next two items we are attempting to recreating the element. A should equal B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate pseudo inverse manually and compare to the built in version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPseudo (curr):\n",
    "    \n",
    "    # calculate svd\n",
    "    U, s, VT = svd(curr)\n",
    "    # reciprocals of s\n",
    "    d = 1.0 / s\n",
    "    # create m x n D matrix\n",
    "    D = np.zeros(curr.shape)\n",
    "    # populate D with n x n diagonal matrix\n",
    "    D[:curr.shape[1], :curr.shape[1]] = np.diag(d)\n",
    "    # calculate pseudoinverse\n",
    "    new = VT.T.dot(D.T).dot(U.T)\n",
    "    \n",
    "    return new\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puilt in PINV function\n",
      "[[ 1.41665154e-06 -7.87624385e-05 -1.50063288e-05 ... -8.81508881e-05\n",
      "  -2.43381414e-04 -1.57186015e-04]\n",
      " [-3.11887480e-05  4.14462675e-05 -2.39004888e-04 ... -1.38122724e-04\n",
      "  -1.99278900e-05 -3.38553861e-04]\n",
      " [-2.30330965e-05 -3.87923977e-05 -8.42779723e-05 ... -1.93302046e-04\n",
      "  -1.29650184e-05  1.44900618e-03]\n",
      " ...\n",
      " [-3.53724560e-07  4.86100109e-07  1.05452792e-06 ...  6.99338964e-07\n",
      "   5.91041688e-07 -3.66002797e-06]\n",
      " [ 4.49952314e-07 -4.67608076e-07 -1.13338360e-07 ... -6.02837625e-07\n",
      "  -5.32801834e-07  3.21050947e-06]\n",
      " [-3.71539162e-09 -5.57172465e-08  5.07890139e-07 ... -1.54623509e-07\n",
      "  -2.89767954e-08 -2.49413822e-07]]\n",
      "Our method\n",
      "[[ 1.41665154e-06 -7.87624385e-05 -1.50063288e-05 ... -8.81508881e-05\n",
      "  -2.43381414e-04 -1.57186015e-04]\n",
      " [-3.11883520e-05  4.14462206e-05 -2.39004961e-04 ... -1.38122710e-04\n",
      "  -1.99278888e-05 -3.38553853e-04]\n",
      " [-2.30322454e-05 -3.87925141e-05 -8.42781799e-05 ... -1.93302006e-04\n",
      "  -1.29647602e-05  1.44900626e-03]\n",
      " ...\n",
      " [-3.53724719e-07  4.86100125e-07  1.05452794e-06 ...  6.99338961e-07\n",
      "   5.91041740e-07 -3.66002796e-06]\n",
      " [ 4.49952818e-07 -4.67608132e-07 -1.13338439e-07 ... -6.02837610e-07\n",
      "  -5.32801893e-07  3.21050947e-06]\n",
      " [-3.71532570e-09 -5.57172542e-08  5.07890127e-07 ... -1.54623507e-07\n",
      "  -2.89767968e-08 -2.49413821e-07]]\n"
     ]
    }
   ],
   "source": [
    "A = train_data\n",
    "\n",
    "# calculate pseudoinverse\n",
    "print('Puilt in PINV function')\n",
    "B = pinv(A)\n",
    "print(B)\n",
    "print('Our method')\n",
    "B=getPseudo(A)\n",
    "print(B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getError(matrix, data, labels):\n",
    "\n",
    "    true = 0\n",
    "    \n",
    "    len(data)\n",
    "    print(len(matrix))\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        x = data[i]\n",
    "        \n",
    "        predict = np.sum(x.dot(matrix))\n",
    "\n",
    "        #find the error\n",
    "        if (labels[i] == False) and (predict<1):\n",
    "            true +=1\n",
    "            #print(predict, ': ', labels[i])\n",
    "        elif (labels[i] == True) and (predict>1):\n",
    "            true+=1\n",
    "       \n",
    "       \n",
    "\n",
    "        #these are  the results from the svd classification\n",
    "        #print(predict, end = \": \")\n",
    "        #print(train_labels[i])\n",
    "\n",
    "        #calculate the error\n",
    "\n",
    "    \n",
    "    return 1-(true/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "error rate:  0.3444126074498567\n",
      "71\n",
      "test rate  0.32699999999999996\n"
     ]
    }
   ],
   "source": [
    "print('error rate: ', getError(B, train_data, train_labels))\n",
    "print('test rate ', getError(B, test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to create a new psuedo inverse matrix. We will exclude all of the genres in the dataset to see if we can get a lower initial error rate.  This matrix will only be a 5*5 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New matrix\n",
      "[[ 849    2    0    1   36]\n",
      " [ 336    0    0    1    7]\n",
      " [ 417   70   20   25  689]\n",
      " ...\n",
      " [ 144    1    1    0    5]\n",
      " [ 460    6    2    0    1]\n",
      " [2975  827  187  255  232]]\n",
      "New Pseudo\n",
      "[[ 2.30179326e-08  9.91475252e-09 -1.34144187e-09 ...  3.27235073e-09\n",
      "   1.03180175e-08  1.56591788e-08]\n",
      " [-2.23284894e-07 -9.99779258e-08 -9.79382441e-08 ... -3.38490848e-08\n",
      "  -8.69787770e-08  6.93662401e-07]\n",
      " [-3.06234988e-07 -1.34135158e-07 -2.48061348e-07 ... -2.21253227e-08\n",
      "  -1.11250122e-07 -3.53011980e-06]\n",
      " [ 7.23994847e-07  3.36342813e-07  7.46779978e-08 ...  8.22741266e-08\n",
      "   2.64910168e-07  2.85604162e-06]\n",
      " [ 1.78131789e-08  7.97238553e-10  5.41029713e-07 ...  2.25907157e-09\n",
      "  -5.47407311e-09 -6.19685001e-08]]\n",
      "5\n",
      "5\n",
      "error rate:  0.8406876790830946\n",
      "5\n",
      "test rate  0.841\n"
     ]
    }
   ],
   "source": [
    "##try to get the psudo inverse when excluding the genre from the list\n",
    "\n",
    "print('New matrix')\n",
    "newA = A[:,-5:]\n",
    "print(newA)\n",
    "print('New Pseudo')\n",
    "newB = getPseudo(newA)\n",
    "print(newB)\n",
    "print(len(newB))\n",
    "\n",
    "print('error rate: ', getError(newB, train_data[:,-5:], train_labels))\n",
    "print('test rate ', getError(newB, test_data[:,-5:], test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compute the derivative of the regression cost function: \n",
    "$$L_D(w) = \\frac{1}{n}\\sum_{i=1}^n (y_i-w\\cdot x_i)^2,$$\n",
    "where $x_i\\in \\mathrm{R}^d$ is the input feature of dimension $d$, $y_i\\in\\mathrm{R}$ is the output response, and $w\\in\\mathrm{R}^d$ is the regression weights.\n",
    "\n",
    "**Task P3:** Complete the function 'weight_derivative' to calculate the derivative of the cost function with respect to regression weights $w$, i.e., $\\frac{\\partial}{\\partial w}L_D(w)$. Note that this should be a $d$ dimensional vector. Also copy the output of the code for the test example to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_derivative(weights, feature_matrix, labels):\n",
    "    # Input:\n",
    "    # weights: weight vector w, a numpy vector of dimension d\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d\n",
    "    # Output:\n",
    "    # Derivative of the regression cost function with respect to the weight w, a numpy array of dimension d\n",
    "        \n",
    "    ## STUDENT: Start of code ###\n",
    "    \n",
    "    test_predictions = np.dot(weights,feature_matrix)\n",
    "    \n",
    "    \n",
    "    \n",
    "    errors = test_predictions - labels\n",
    "    \n",
    "    derivative = np.dot(feature_matrix,np.transpose(errors))\n",
    "    derivative = derivative#/(np.sum(derivative))\n",
    "    return(derivative)\n",
    "    # End of code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.04014813e-03  8.46879732e-03 -1.75732122e-02 -4.83218283e-03\n",
      "  3.31761432e-04  3.86677752e-03 -1.78805059e-02 -8.96592632e-03\n",
      "  1.26142539e-02  9.93006212e-04 -6.79400073e-03 -2.14297097e-04\n",
      "  3.06249437e-03 -2.12486455e-03 -3.32189037e-03 -9.25336117e-05\n",
      " -5.08664039e-03  5.34560516e+06  1.46648453e-02  8.18348156e-03\n",
      " -8.81335130e-03  1.77514288e-02  1.07599593e-02  2.09512774e-03\n",
      "  7.10672677e-03 -2.64451790e-03 -1.96833512e-02  1.21160566e-02\n",
      " -5.79651531e-03  7.60269106e-03 -9.53161241e-03 -5.25935632e-04\n",
      " -2.48322560e-02 -1.18838144e-02 -4.99126297e-03  7.36531047e-03\n",
      "  1.69510026e-02 -1.88833970e-02  1.03962200e-02 -9.69900785e-03\n",
      " -1.72892240e-03 -7.62163761e-03 -1.56968660e-02  4.91079001e-03\n",
      "  2.18878440e-02  7.87371709e-03 -1.52332115e-02 -5.49918155e-03\n",
      "  2.76562561e-04  2.55078104e-02 -7.38967766e-03  1.57961186e+08\n",
      "  1.91096773e-05 -2.73740747e-03  3.23087238e-04 -3.96663658e-04\n",
      " -2.46739209e-03 -1.31894691e-01  7.02742986e-03 -6.61882453e-01\n",
      " -2.21172657e-01  6.31141098e-04 -6.47370123e-01 -1.98541317e-01\n",
      "  1.62171082e-02 -2.27480568e-08 -3.70773514e-07 -4.45994720e-05\n",
      "  1.61203871e-04 -4.00461037e-05  1.31826304e-06]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: copy the output to the solution file.\n",
    "\n",
    "\n",
    "\n",
    "my_weights = np.zeros(len(train_data[0])) # this makes all the predictions 0\n",
    "derivative = weight_derivative(my_weights, B, train_labels)\n",
    "\n",
    "print (derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will write a function to perform gradient descent algorithm on the lineare regression cost. Given an initial point, we will update the current weights by moving in the negative gradient direction to minimize the cost function. Thus, in each iteration we obtain the updated weight $w_{t+1}$ from the current iterate $w_t$ as follows:\n",
    "$$w_{t+1} = w_t - h\\frac{\\partial}{\\partial w}L_D(w_t),$$\n",
    "where $h$ is the 'step_size' that is the amount by which we move in the negative gradient direction. \n",
    "\n",
    "We stop when we are sufficiently close to the optimum (where gradient is the zero vector) by checking the condition with respect to the magnitude (length) of the gradient vector:\n",
    "$$\\|\\frac{\\partial}{\\partial w}L_D(w_t)\\|_2\\leq \\epsilon,$$\n",
    "where $\\epsilon$ is the 'tolerance' parameter.\n",
    "\n",
    "**Task P4:** Complete the code section to perform the gradient decent in the function `regression_gradient_descent`. Copy the code to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, labels, initial_weights, step_size, tolerance):\n",
    "    # Gradient descent algorithm for linear regression problem    \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d\n",
    "    # initial_weights: initial weight vector to start with, a numpy vector of dimension d\n",
    "    # step_size: step size of update\n",
    "    # tolerance: tolerace epsilon for stopping condition\n",
    "    # Output:\n",
    "    # Weights obtained after convergence\n",
    "    \n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # current iterate\n",
    "    i = 0\n",
    "    while not converged:\n",
    "        # Start of code: your impelementation of what the gradient descent algorithm does in every iteration\n",
    "        # Refer back to the update rule listed above: update the weight\n",
    "        i += 1\n",
    "        derivative = weight_derivative(weights, feature_matrix, labels)\n",
    "        #print(derivative)\n",
    "        weights -= (step_size * derivative)\n",
    "        \n",
    "        # Compute the gradient magnitude:\n",
    "        \n",
    "        gradient_magnitude = np.sqrt(np.sum(derivative**2))\n",
    "        \n",
    "        # Check the stopping condition to decide whether you want to stop the iterations\n",
    "        #print (\"grad mag :\", gradient_magnitude)\n",
    "        #print (\"tolerance:\", tolerance)\n",
    "        if gradient_magnitude > tolerance:\n",
    "            converged = True\n",
    "        \n",
    "        # End of code\n",
    "        \n",
    "        print (\"Iteration: \",i,\"gradient_magnitude: \", gradient_magnitude) # for us to check about convergence\n",
    "        x = np.dot(np.transpose(weights),B)\n",
    "        print('error rate: ', getError(x, train_data[:2000], train_labels[:2000]))\n",
    "\n",
    "        #print('error rate: ', getError(x, train_data, train_labels))\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1 gradient_magnitude:  3.8999877318054246e+17\n",
      "71\n",
      "error rate:  0.11950000000000005\n",
      "Here are the final weights after convergence:\n",
      "[[ 0.99852215  0.99852215  0.99852215 ...  0.99852215  0.99852215\n",
      "   0.99852215]\n",
      " [ 4.14749385  4.14749385  4.14749385 ...  4.14749385  4.14749385\n",
      "   4.14749385]\n",
      " [20.3919445  20.3919445  20.3919445  ... 20.3919445  20.3919445\n",
      "  20.3919445 ]\n",
      " ...\n",
      " [ 1.00123865  1.00123865  1.00123865 ...  1.00123865  1.00123865\n",
      "   1.00123865]\n",
      " [ 1.00103201  1.00103201  1.00103201 ...  1.00103201  1.00103201\n",
      "   1.00103201]\n",
      " [ 1.00044657  1.00044657  1.00044657 ...  1.00044657  1.00044657\n",
      "   1.00044657]]\n"
     ]
    }
   ],
   "source": [
    "#STUDENT: Specify the initial_weights, step_size, and tolerance\n",
    "simple_feature_matrix = B\n",
    "output = train_labels\n",
    "\n",
    "\n",
    "temp_weights = [np.ones(len(train_data[0]))]*(len(train_data[0]))# this makes all the predictions 0 #np.array([0.]*2*len(simple_feature_matrix))\n",
    "temp_weights=np.array(temp_weights)\n",
    "initial_weights = temp_weights.reshape(len(simple_feature_matrix),len(train_data[0]))\n",
    "\n",
    "step_size = 0.5\n",
    "tolerance = 2\n",
    "# end of code\n",
    "\n",
    "# Use the regression_gradient_descent function to calculate the gradient decent and store it in the variable 'final_weights'\n",
    "final_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)\n",
    "\n",
    "# end of code\n",
    "print (\"Here are the final weights after convergence:\")\n",
    "\n",
    "print (final_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15705\n",
      "71\n",
      "71\n",
      "71\n",
      "error rate:  0.1217446673034066\n",
      "71\n",
      "test rate:  0.10899999999999999\n"
     ]
    }
   ],
   "source": [
    "#print(len(B[0]))\n",
    "#print(len(final_weights[0]))\n",
    "\n",
    "x = np.dot(np.transpose(final_weights),B)\n",
    "\n",
    "#print(len(x))\n",
    "print('error rate: ', getError(x, train_data, train_labels))\n",
    "print('test rate: ', getError(x, test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
