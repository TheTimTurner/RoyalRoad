{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import random\n",
    "#!pip install ProgressBar\n",
    "from progressbar import ProgressBar\n",
    "import math\n",
    "#!{sys.executable} -m pip install ProgressBar\n",
    "#!pip install ProgressBar\n",
    "from random import randrange\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "#svd stuff\n",
    "from scipy.linalg import svd\n",
    "from numpy.linalg import pinv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the titles and wheater the book is alive or dead into two seperate arrays\n",
    "def loadResult():\n",
    "    test_labels = []\n",
    "    test_values = []\n",
    "    with open('table-of-contents.json', encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "        for books in data:\n",
    "            for book in data[books]:\n",
    "                test_labels.append(book['title'])\n",
    "                test_values.append(book['dead'])\n",
    "                \n",
    "    return test_labels, test_values\n",
    "\n",
    "#get the data from the data output file\n",
    "def loadData():\n",
    "    final_data = []\n",
    "    with open('dataOutput2.json', encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "        for index in data: \n",
    "            final_data.append(data[index]['chapData'])\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16705\n",
      "Training dataset dimensions:  (15705, 71)\n",
      "Number of training labels:  15705\n",
      "Testing dataset dimensions:  (1000, 71)\n",
      "Number of testing labels:  1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                \n",
    "\n",
    "title_labels, dead_labels = loadResult()\n",
    "train = loadData()\n",
    "print(len(train))\n",
    "## Load the training set\n",
    "train_data= np.array(train[:15705])\n",
    "train_labels = np.array(dead_labels[:15705])\n",
    "train_titles = np.array(title_labels[:15705])\n",
    "\n",
    "\n",
    "## Load the testing set\n",
    "\n",
    "test_data= np.array(train[-1000:])\n",
    "test_labels = np.array(dead_labels[-1000:])\n",
    "test_titles = np.array(title_labels[-1000:])\n",
    "\n",
    "\n",
    "## Print out their dimensions\n",
    "print(\"Training dataset dimensions: \", np.shape(train_data))\n",
    "print(\"Number of training labels: \", len(train_labels))\n",
    "print(\"Testing dataset dimensions: \", np.shape(test_data))\n",
    "print(\"Number of testing labels: \", len(test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next two items we are attempting to recreating the element. A should equal B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate pseudo inverse manually and compare to the built in version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   1  36]\n",
      " [  0   0   0 ...   0   1   7]\n",
      " [  0   0   0 ...  20  25 689]\n",
      " ...\n",
      " [  0   0   0 ...   1   0   5]\n",
      " [  0   0   0 ...   2   0   1]\n",
      " [  0   0   1 ... 187 255 232]]\n",
      "[[ 1.41665154e-06 -7.87624385e-05 -1.50063288e-05 ... -8.81508881e-05\n",
      "  -2.43381414e-04 -1.57186015e-04]\n",
      " [-3.11887480e-05  4.14462675e-05 -2.39004888e-04 ... -1.38122724e-04\n",
      "  -1.99278900e-05 -3.38553861e-04]\n",
      " [-2.30330965e-05 -3.87923977e-05 -8.42779723e-05 ... -1.93302046e-04\n",
      "  -1.29650184e-05  1.44900618e-03]\n",
      " ...\n",
      " [-3.53724560e-07  4.86100109e-07  1.05452792e-06 ...  6.99338964e-07\n",
      "   5.91041688e-07 -3.66002797e-06]\n",
      " [ 4.49952314e-07 -4.67608076e-07 -1.13338360e-07 ... -6.02837625e-07\n",
      "  -5.32801834e-07  3.21050947e-06]\n",
      " [-3.71539162e-09 -5.57172465e-08  5.07890139e-07 ... -1.54623509e-07\n",
      "  -2.89767954e-08 -2.49413822e-07]]\n"
     ]
    }
   ],
   "source": [
    "#A= np.array([[0.1, 0.2],[0.3, 0.4],[0.5, 0.6],[0.7, 0.8]])\n",
    "A=np.array(train_data[:])\n",
    "print(A)\n",
    "# calculate pseudoinverse\n",
    "B = pinv(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   1  36]\n",
      " [  0   0   0 ...   0   1   7]\n",
      " [  0   0   0 ...  20  25 689]\n",
      " ...\n",
      " [  0   0   0 ...   1   0   5]\n",
      " [  0   0   0 ...   2   0   1]\n",
      " [  0   0   1 ... 187 255 232]]\n",
      "[[ 1.41665154e-06 -7.87624385e-05 -1.50063288e-05 ... -8.81508881e-05\n",
      "  -2.43381414e-04 -1.57186015e-04]\n",
      " [-3.11883520e-05  4.14462206e-05 -2.39004961e-04 ... -1.38122710e-04\n",
      "  -1.99278888e-05 -3.38553853e-04]\n",
      " [-2.30322454e-05 -3.87925141e-05 -8.42781799e-05 ... -1.93302006e-04\n",
      "  -1.29647602e-05  1.44900626e-03]\n",
      " ...\n",
      " [-3.53724719e-07  4.86100125e-07  1.05452794e-06 ...  6.99338961e-07\n",
      "   5.91041740e-07 -3.66002796e-06]\n",
      " [ 4.49952818e-07 -4.67608132e-07 -1.13338439e-07 ... -6.02837610e-07\n",
      "  -5.32801893e-07  3.21050947e-06]\n",
      " [-3.71532570e-09 -5.57172542e-08  5.07890127e-07 ... -1.54623507e-07\n",
      "  -2.89767968e-08 -2.49413821e-07]]\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "# calculate svd\n",
    "U, s, VT = svd(A)\n",
    "# reciprocals of s\n",
    "d = 1.0 / s\n",
    "# create m x n D matrix\n",
    "D = np.zeros(A.shape)\n",
    "# populate D with n x n diagonal matrix\n",
    "D[:A.shape[1], :A.shape[1]] = np.diag(d)\n",
    "# calculate pseudoinverse\n",
    "B = VT.T.dot(D.T).dot(U.T)\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getError(B, data, labels):\n",
    "\n",
    "    true = 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        x = data[i]\n",
    "        predict = np.sum(x.dot(B))\n",
    "\n",
    "        #find the error\n",
    "        if (labels[i] == False) and (predict<1):\n",
    "            true +=1\n",
    "        elif (labels[i] == True) and (predict>1):\n",
    "            true+=1\n",
    "\n",
    "\n",
    "\n",
    "        #these are  the results from the svd classification\n",
    "        #print(predict, end = \": \")\n",
    "        #print(train_labels[i])\n",
    "\n",
    "        #calculate the error\n",
    "\n",
    "    #print('error rate: ', 1-(true/len(train_data)))\n",
    "    return 1-(true/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate:  0.3444126074498567\n",
      "test rate  0.32699999999999996\n"
     ]
    }
   ],
   "source": [
    "print('error rate: ', getError(B, train_data, train_labels))\n",
    "print('test rate ', getError(B, test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compute the derivative of the regression cost function: \n",
    "$$L_D(w) = \\frac{1}{n}\\sum_{i=1}^n (y_i-w\\cdot x_i)^2,$$\n",
    "where $x_i\\in \\mathrm{R}^d$ is the input feature of dimension $d$, $y_i\\in\\mathrm{R}$ is the output response, and $w\\in\\mathrm{R}^d$ is the regression weights.\n",
    "\n",
    "**Task P3:** Complete the function 'weight_derivative' to calculate the derivative of the cost function with respect to regression weights $w$, i.e., $\\frac{\\partial}{\\partial w}L_D(w)$. Note that this should be a $d$ dimensional vector. Also copy the output of the code for the test example to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_derivative(weights, feature_matrix, labels):\n",
    "    # Input:\n",
    "    # weights: weight vector w, a numpy vector of dimension d\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d\n",
    "    # Output:\n",
    "    # Derivative of the regression cost function with respect to the weight w, a numpy array of dimension d\n",
    "        \n",
    "    ## STUDENT: Start of code ###\n",
    "    \n",
    "    test_predictions = np.dot(weights,feature_matrix)\n",
    "    \n",
    "    \n",
    "    \n",
    "    errors = test_predictions - labels\n",
    "    \n",
    "    derivative = np.dot(feature_matrix,np.transpose(errors))\n",
    "    derivative = derivative/(np.sum(derivative))\n",
    "    return(derivative)\n",
    "    # End of code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.51555872e-20 -9.61713742e-17 -5.92519013e-16 -1.28320228e-16\n",
      "  6.13282822e-17 -3.27404760e-16 -6.71135514e-17 -2.62725546e-17\n",
      "  2.96568709e-16 -4.83750358e-17  9.26317472e-17  2.38855197e-16\n",
      " -1.73723779e-16 -2.66681840e-17 -9.98688248e-17 -3.34050930e-16\n",
      " -2.85039114e-16  5.00000301e-01 -8.11113977e-16 -3.20325316e-16\n",
      " -2.43297182e-16  2.84696476e-16  1.78917409e-16 -1.37586376e-18\n",
      " -2.33402190e-16  5.90503838e-16 -3.76530538e-16 -5.97658498e-16\n",
      "  6.23338534e-17  2.16237201e-16  6.39826970e-16 -4.05688942e-17\n",
      "  2.31526933e-16 -4.10611312e-16 -3.75145921e-16 -6.39388390e-16\n",
      "  7.02236553e-16 -4.19627134e-16 -1.57378623e-17 -1.57886652e-16\n",
      "  1.28766053e-16 -4.41106025e-16  1.72990441e-16 -5.45188641e-17\n",
      " -1.44584219e-16 -1.16906387e-18 -9.69117582e-17  8.52081894e-17\n",
      " -1.15959261e-16  4.14856549e-16  2.35172768e-16  4.99999699e-01\n",
      "  2.70075316e-16  2.81446554e-16 -7.48279612e-18 -9.60986243e-17\n",
      " -1.59045778e-16 -3.96531751e-16  2.71607571e-16  5.36385081e-16\n",
      " -3.54320512e-16 -5.35683111e-16  2.67451459e-16 -5.14516879e-16\n",
      " -1.87248218e-16  4.14163202e-24  2.98900439e-21  3.01174517e-20\n",
      " -3.78467028e-20 -3.15329009e-20 -1.36449141e-20]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: copy the output to the solution file.\n",
    "\n",
    "\n",
    "\n",
    "my_weights = np.ones(len(train_data[0])) # this makes all the predictions 0\n",
    "derivative = weight_derivative(my_weights, B, train_labels)\n",
    "\n",
    "print (derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will write a function to perform gradient descent algorithm on the lineare regression cost. Given an initial point, we will update the current weights by moving in the negative gradient direction to minimize the cost function. Thus, in each iteration we obtain the updated weight $w_{t+1}$ from the current iterate $w_t$ as follows:\n",
    "$$w_{t+1} = w_t - h\\frac{\\partial}{\\partial w}L_D(w_t),$$\n",
    "where $h$ is the 'step_size' that is the amount by which we move in the negative gradient direction. \n",
    "\n",
    "We stop when we are sufficiently close to the optimum (where gradient is the zero vector) by checking the condition with respect to the magnitude (length) of the gradient vector:\n",
    "$$\\|\\frac{\\partial}{\\partial w}L_D(w_t)\\|_2\\leq \\epsilon,$$\n",
    "where $\\epsilon$ is the 'tolerance' parameter.\n",
    "\n",
    "**Task P4:** Complete the code section to perform the gradient decent in the function `regression_gradient_descent`. Copy the code to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, labels, initial_weights, step_size, tolerance):\n",
    "    # Gradient descent algorithm for linear regression problem    \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d\n",
    "    # initial_weights: initial weight vector to start with, a numpy vector of dimension d\n",
    "    # step_size: step size of update\n",
    "    # tolerance: tolerace epsilon for stopping condition\n",
    "    # Output:\n",
    "    # Weights obtained after convergence\n",
    "    \n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # current iterate\n",
    "    i = 0\n",
    "    while not converged:\n",
    "        # Start of code: your impelementation of what the gradient descent algorithm does in every iteration\n",
    "        # Refer back to the update rule listed above: update the weight\n",
    "        i += 1\n",
    "        derivative = weight_derivative(weights, feature_matrix, labels)\n",
    "        #print(derivative)\n",
    "        weights -= (step_size * derivative)\n",
    "        \n",
    "        # Compute the gradient magnitude:\n",
    "        \n",
    "        gradient_magnitude = np.sqrt(np.sum(derivative**2))\n",
    "        \n",
    "        # Check the stopping condition to decide whether you want to stop the iterations\n",
    "        #print (\"grad mag :\", gradient_magnitude)\n",
    "        #print (\"tolerance:\", tolerance)\n",
    "        if gradient_magnitude > tolerance:\n",
    "            converged = True\n",
    "        \n",
    "        # End of code\n",
    "        \n",
    "        print (\"Iteration: \",i,\"gradient_magnitude: \", gradient_magnitude) # for us to check about convergence\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1 gradient_magnitude:  0.08391813582968449\n",
      "Iteration:  2 gradient_magnitude:  0.08391819281772238\n",
      "Iteration:  3 gradient_magnitude:  0.0839183623131415\n",
      "Iteration:  4 gradient_magnitude:  0.08391864213559787\n",
      "Iteration:  5 gradient_magnitude:  0.08391903013476107\n",
      "Iteration:  6 gradient_magnitude:  0.08391952419000204\n",
      "Iteration:  7 gradient_magnitude:  0.08392012221008241\n",
      "Iteration:  8 gradient_magnitude:  0.08392082213284541\n",
      "Iteration:  9 gradient_magnitude:  0.08392162192490843\n",
      "Iteration:  10 gradient_magnitude:  0.08392251958135717\n",
      "Iteration:  11 gradient_magnitude:  0.08392351312544144\n",
      "Iteration:  12 gradient_magnitude:  0.08392460060827248\n",
      "Iteration:  13 gradient_magnitude:  0.08392578010852236\n",
      "Iteration:  14 gradient_magnitude:  0.08392704973212478\n",
      "Iteration:  15 gradient_magnitude:  0.0839284076119778\n",
      "Iteration:  16 gradient_magnitude:  0.0839298519076484\n",
      "Iteration:  17 gradient_magnitude:  0.08393138080507859\n",
      "Iteration:  18 gradient_magnitude:  0.08393299251629377\n",
      "Iteration:  19 gradient_magnitude:  0.0839346852791127\n",
      "Iteration:  20 gradient_magnitude:  0.08393645735685926\n",
      "Iteration:  21 gradient_magnitude:  0.08393830703807642\n",
      "Iteration:  22 gradient_magnitude:  0.0839402326362418\n",
      "Iteration:  23 gradient_magnitude:  0.08394223248948551\n",
      "Iteration:  24 gradient_magnitude:  0.08394430496030958\n",
      "Iteration:  25 gradient_magnitude:  0.08394644843530973\n",
      "Iteration:  26 gradient_magnitude:  0.08394866132489871\n",
      "Iteration:  27 gradient_magnitude:  0.08395094206303218\n",
      "Iteration:  28 gradient_magnitude:  0.08395328910693595\n",
      "Iteration:  29 gradient_magnitude:  0.08395570093683585\n",
      "Iteration:  30 gradient_magnitude:  0.08395817605568927\n",
      "Iteration:  31 gradient_magnitude:  0.08396071298891886\n",
      "Iteration:  32 gradient_magnitude:  0.08396331028414822\n",
      "Iteration:  33 gradient_magnitude:  0.0839659665109398\n",
      "Iteration:  34 gradient_magnitude:  0.0839686802605347\n",
      "Iteration:  35 gradient_magnitude:  0.08397145014559462\n",
      "Iteration:  36 gradient_magnitude:  0.08397427479994599\n",
      "Iteration:  37 gradient_magnitude:  0.08397715287832587\n",
      "Iteration:  38 gradient_magnitude:  0.08398008305613043\n",
      "Iteration:  39 gradient_magnitude:  0.08398306402916508\n",
      "Iteration:  40 gradient_magnitude:  0.08398609451339689\n",
      "Iteration:  41 gradient_magnitude:  0.08398917324470916\n",
      "Iteration:  42 gradient_magnitude:  0.08399229897865797\n",
      "Iteration:  43 gradient_magnitude:  0.08399547049023096\n",
      "Iteration:  44 gradient_magnitude:  0.08399868657360808\n",
      "Iteration:  45 gradient_magnitude:  0.0840019460419246\n",
      "Iteration:  46 gradient_magnitude:  0.084005247727036\n",
      "Iteration:  47 gradient_magnitude:  0.08400859047928529\n",
      "Iteration:  48 gradient_magnitude:  0.08401197316727205\n",
      "Iteration:  49 gradient_magnitude:  0.08401539467762396\n",
      "Iteration:  50 gradient_magnitude:  0.08401885391477007\n",
      "Iteration:  51 gradient_magnitude:  0.08402234980071639\n",
      "Iteration:  52 gradient_magnitude:  0.08402588127482355\n",
      "Iteration:  53 gradient_magnitude:  0.08402944729358641\n",
      "Iteration:  54 gradient_magnitude:  0.08403304683041596\n",
      "Iteration:  55 gradient_magnitude:  0.08403667887542304\n",
      "Iteration:  56 gradient_magnitude:  0.0840403424352044\n",
      "Iteration:  57 gradient_magnitude:  0.08404403653263061\n",
      "Iteration:  58 gradient_magnitude:  0.08404776020663621\n",
      "Iteration:  59 gradient_magnitude:  0.08405151251201168\n",
      "Iteration:  60 gradient_magnitude:  0.0840552925191977\n",
      "Iteration:  61 gradient_magnitude:  0.08405909931408141\n",
      "Iteration:  62 gradient_magnitude:  0.08406293199779444\n",
      "Iteration:  63 gradient_magnitude:  0.08406678968651328\n",
      "Iteration:  64 gradient_magnitude:  0.08407067151126156\n",
      "Iteration:  65 gradient_magnitude:  0.08407457661771428\n",
      "Iteration:  66 gradient_magnitude:  0.08407850416600408\n",
      "Iteration:  67 gradient_magnitude:  0.08408245333052951\n",
      "Iteration:  68 gradient_magnitude:  0.0840864232997652\n",
      "Iteration:  69 gradient_magnitude:  0.08409041327607418\n",
      "Iteration:  70 gradient_magnitude:  0.08409442247552197\n",
      "Iteration:  71 gradient_magnitude:  0.08409845012769264\n",
      "Iteration:  72 gradient_magnitude:  0.08410249547550704\n",
      "Iteration:  73 gradient_magnitude:  0.08410655777504253\n",
      "Iteration:  74 gradient_magnitude:  0.08411063629535512\n",
      "Iteration:  75 gradient_magnitude:  0.08411473031830312\n",
      "Iteration:  76 gradient_magnitude:  0.08411883913837287\n",
      "Iteration:  77 gradient_magnitude:  0.0841229620625064\n",
      "Iteration:  78 gradient_magnitude:  0.08412709840993077\n",
      "Iteration:  79 gradient_magnitude:  0.08413124751198953\n",
      "Iteration:  80 gradient_magnitude:  0.08413540871197572\n",
      "Iteration:  81 gradient_magnitude:  0.08413958136496702\n",
      "Iteration:  82 gradient_magnitude:  0.08414376483766255\n",
      "Iteration:  83 gradient_magnitude:  0.08414795850822146\n",
      "Iteration:  84 gradient_magnitude:  0.08415216176610345\n",
      "Iteration:  85 gradient_magnitude:  0.08415637401191092\n",
      "Iteration:  86 gradient_magnitude:  0.08416059465723308\n",
      "Iteration:  87 gradient_magnitude:  0.08416482312449168\n",
      "Iteration:  88 gradient_magnitude:  0.08416905884678855\n",
      "Iteration:  89 gradient_magnitude:  0.08417330126775475\n",
      "Iteration:  90 gradient_magnitude:  0.08417754984140179\n",
      "Iteration:  91 gradient_magnitude:  0.08418180403197413\n",
      "Iteration:  92 gradient_magnitude:  0.08418606331380371\n",
      "Iteration:  93 gradient_magnitude:  0.08419032717116592\n",
      "Iteration:  94 gradient_magnitude:  0.08419459509813745\n",
      "Iteration:  95 gradient_magnitude:  0.08419886659845571\n",
      "Iteration:  96 gradient_magnitude:  0.0842031411853798\n",
      "Iteration:  97 gradient_magnitude:  0.08420741838155336\n",
      "Iteration:  98 gradient_magnitude:  0.08421169771886874\n",
      "Iteration:  99 gradient_magnitude:  0.08421597873833307\n",
      "Iteration:  100 gradient_magnitude:  0.08422026098993557\n",
      "Iteration:  101 gradient_magnitude:  0.08422454403251692\n",
      "Iteration:  102 gradient_magnitude:  0.08422882743363969\n",
      "Iteration:  103 gradient_magnitude:  0.0842331107694607\n",
      "Iteration:  104 gradient_magnitude:  0.08423739362460472\n",
      "Iteration:  105 gradient_magnitude:  0.08424167559203975\n",
      "Iteration:  106 gradient_magnitude:  0.08424595627295385\n",
      "Iteration:  107 gradient_magnitude:  0.08425023527663336\n",
      "Iteration:  108 gradient_magnitude:  0.0842545122203428\n",
      "Iteration:  109 gradient_magnitude:  0.08425878672920598\n",
      "Iteration:  110 gradient_magnitude:  0.08426305843608882\n",
      "Iteration:  111 gradient_magnitude:  0.0842673269814835\n",
      "Iteration:  112 gradient_magnitude:  0.08427159201339397\n",
      "Iteration:  113 gradient_magnitude:  0.08427585318722304\n",
      "Iteration:  114 gradient_magnitude:  0.08428011016566074\n",
      "Iteration:  115 gradient_magnitude:  0.08428436261857412\n",
      "Iteration:  116 gradient_magnitude:  0.0842886102228985\n",
      "Iteration:  117 gradient_magnitude:  0.08429285266252996\n",
      "Iteration:  118 gradient_magnitude:  0.08429708962821923\n",
      "Iteration:  119 gradient_magnitude:  0.084301320817467\n",
      "Iteration:  120 gradient_magnitude:  0.0843055459344204\n",
      "Iteration:  121 gradient_magnitude:  0.08430976468977096\n",
      "Iteration:  122 gradient_magnitude:  0.08431397680065368\n",
      "Iteration:  123 gradient_magnitude:  0.0843181819905477\n",
      "Iteration:  124 gradient_magnitude:  0.08432237998917781\n",
      "Iteration:  125 gradient_magnitude:  0.08432657053241759\n",
      "Iteration:  126 gradient_magnitude:  0.08433075336219362\n",
      "Iteration:  127 gradient_magnitude:  0.08433492822639095\n",
      "Iteration:  128 gradient_magnitude:  0.0843390948787599\n",
      "Iteration:  129 gradient_magnitude:  0.08434325307882384\n",
      "Iteration:  130 gradient_magnitude:  0.08434740259178844\n",
      "Iteration:  131 gradient_magnitude:  0.08435154318845194\n",
      "Iteration:  132 gradient_magnitude:  0.08435567464511654\n",
      "Iteration:  133 gradient_magnitude:  0.08435979674350116\n",
      "Iteration:  134 gradient_magnitude:  0.08436390927065517\n",
      "Iteration:  135 gradient_magnitude:  0.08436801201887333\n",
      "Iteration:  136 gradient_magnitude:  0.08437210478561177\n",
      "Iteration:  137 gradient_magnitude:  0.08437618737340533\n",
      "Iteration:  138 gradient_magnitude:  0.08438025958978561\n",
      "Iteration:  139 gradient_magnitude:  0.08438432124720043\n",
      "Iteration:  140 gradient_magnitude:  0.08438837216293427\n",
      "Iteration:  141 gradient_magnitude:  0.08439241215902969\n",
      "Iteration:  142 gradient_magnitude:  0.08439644106220993\n",
      "Iteration:  143 gradient_magnitude:  0.0844004587038025\n",
      "Iteration:  144 gradient_magnitude:  0.08440446491966373\n",
      "Iteration:  145 gradient_magnitude:  0.08440845955010448\n",
      "Iteration:  146 gradient_magnitude:  0.08441244243981672\n",
      "Iteration:  147 gradient_magnitude:  0.08441641343780122\n",
      "Iteration:  148 gradient_magnitude:  0.08442037239729609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  149 gradient_magnitude:  0.08442431917570652\n",
      "Iteration:  150 gradient_magnitude:  0.08442825363453513\n",
      "Iteration:  151 gradient_magnitude:  0.08443217563931367\n",
      "Iteration:  152 gradient_magnitude:  0.08443608505953538\n",
      "Iteration:  153 gradient_magnitude:  0.08443998176858837\n",
      "Iteration:  154 gradient_magnitude:  0.0844438656436899\n",
      "Iteration:  155 gradient_magnitude:  0.08444773656582162\n",
      "Iteration:  156 gradient_magnitude:  0.0844515944196657\n",
      "Iteration:  157 gradient_magnitude:  0.08445543909354176\n",
      "Iteration:  158 gradient_magnitude:  0.08445927047934479\n",
      "Iteration:  159 gradient_magnitude:  0.08446308847248381\n",
      "Iteration:  160 gradient_magnitude:  0.08446689297182156\n",
      "Iteration:  161 gradient_magnitude:  0.08447068387961489\n",
      "Iteration:  162 gradient_magnitude:  0.084474461101456\n",
      "Iteration:  163 gradient_magnitude:  0.0844782245462146\n",
      "Iteration:  164 gradient_magnitude:  0.08448197412598087\n",
      "Iteration:  165 gradient_magnitude:  0.08448570975600903\n",
      "Iteration:  166 gradient_magnitude:  0.08448943135466198\n",
      "Iteration:  167 gradient_magnitude:  0.08449313884335666\n",
      "Iteration:  168 gradient_magnitude:  0.08449683214650995\n",
      "Iteration:  169 gradient_magnitude:  0.08450051119148576\n",
      "Iteration:  170 gradient_magnitude:  0.08450417590854249\n",
      "Iteration:  171 gradient_magnitude:  0.08450782623078144\n",
      "Iteration:  172 gradient_magnitude:  0.08451146209409591\n",
      "Iteration:  173 gradient_magnitude:  0.08451508343712108\n",
      "Iteration:  174 gradient_magnitude:  0.08451869020118456\n",
      "Iteration:  175 gradient_magnitude:  0.08452228233025762\n",
      "Iteration:  176 gradient_magnitude:  0.08452585977090725\n",
      "Iteration:  177 gradient_magnitude:  0.0845294224722488\n",
      "Iteration:  178 gradient_magnitude:  0.08453297038589934\n",
      "Iteration:  179 gradient_magnitude:  0.08453650346593172\n",
      "Iteration:  180 gradient_magnitude:  0.08454002166882932\n",
      "Iteration:  181 gradient_magnitude:  0.08454352495344136\n",
      "Iteration:  182 gradient_magnitude:  0.08454701328093905\n",
      "Iteration:  183 gradient_magnitude:  0.08455048661477216\n",
      "Iteration:  184 gradient_magnitude:  0.08455394492062637\n",
      "Iteration:  185 gradient_magnitude:  0.0845573881663813\n",
      "Iteration:  186 gradient_magnitude:  0.08456081632206895\n",
      "Iteration:  187 gradient_magnitude:  0.08456422935983303\n",
      "Iteration:  188 gradient_magnitude:  0.08456762725388861\n",
      "Iteration:  189 gradient_magnitude:  0.08457100998048263\n",
      "Iteration:  190 gradient_magnitude:  0.08457437751785482\n",
      "Iteration:  191 gradient_magnitude:  0.08457772984619932\n",
      "Iteration:  192 gradient_magnitude:  0.08458106694762667\n",
      "Iteration:  193 gradient_magnitude:  0.08458438880612676\n",
      "Iteration:  194 gradient_magnitude:  0.08458769540753189\n",
      "Iteration:  195 gradient_magnitude:  0.08459098673948075\n",
      "Iteration:  196 gradient_magnitude:  0.08459426279138266\n",
      "Iteration:  197 gradient_magnitude:  0.08459752355438253\n",
      "Iteration:  198 gradient_magnitude:  0.08460076902132639\n",
      "Iteration:  199 gradient_magnitude:  0.0846039991867272\n",
      "Iteration:  200 gradient_magnitude:  0.08460721404673147\n",
      "Iteration:  201 gradient_magnitude:  0.08461041359908616\n",
      "Iteration:  202 gradient_magnitude:  0.08461359784310632\n",
      "Iteration:  203 gradient_magnitude:  0.08461676677964292\n",
      "Iteration:  204 gradient_magnitude:  0.08461992041105143\n",
      "Iteration:  205 gradient_magnitude:  0.08462305874116087\n",
      "Iteration:  206 gradient_magnitude:  0.08462618177524316\n",
      "Iteration:  207 gradient_magnitude:  0.08462928951998307\n",
      "Iteration:  208 gradient_magnitude:  0.08463238198344865\n",
      "Iteration:  209 gradient_magnitude:  0.08463545917506209\n",
      "Iteration:  210 gradient_magnitude:  0.084638521105571\n",
      "Iteration:  211 gradient_magnitude:  0.0846415677870202\n",
      "Iteration:  212 gradient_magnitude:  0.08464459923272387\n",
      "Iteration:  213 gradient_magnitude:  0.08464761545723831\n",
      "Iteration:  214 gradient_magnitude:  0.08465061647633478\n",
      "Iteration:  215 gradient_magnitude:  0.08465360230697327\n",
      "Iteration:  216 gradient_magnitude:  0.0846565729672762\n",
      "Iteration:  217 gradient_magnitude:  0.08465952847650281\n",
      "Iteration:  218 gradient_magnitude:  0.08466246885502392\n",
      "Iteration:  219 gradient_magnitude:  0.08466539412429702\n",
      "Iteration:  220 gradient_magnitude:  0.08466830430684182\n",
      "Iteration:  221 gradient_magnitude:  0.08467119942621613\n",
      "Iteration:  222 gradient_magnitude:  0.08467407950699223\n",
      "Iteration:  223 gradient_magnitude:  0.08467694457473347\n",
      "Iteration:  224 gradient_magnitude:  0.08467979465597145\n",
      "Iteration:  225 gradient_magnitude:  0.08468262977818326\n",
      "Iteration:  226 gradient_magnitude:  0.0846854499697695\n",
      "Iteration:  227 gradient_magnitude:  0.0846882552600322\n",
      "Iteration:  228 gradient_magnitude:  0.08469104567915352\n",
      "Iteration:  229 gradient_magnitude:  0.08469382125817454\n",
      "Iteration:  230 gradient_magnitude:  0.08469658202897436\n",
      "Iteration:  231 gradient_magnitude:  0.08469932802424983\n",
      "Iteration:  232 gradient_magnitude:  0.08470205927749526\n",
      "Iteration:  233 gradient_magnitude:  0.08470477582298272\n",
      "Iteration:  234 gradient_magnitude:  0.08470747769574252\n",
      "Iteration:  235 gradient_magnitude:  0.08471016493154414\n",
      "Iteration:  236 gradient_magnitude:  0.08471283756687731\n",
      "Iteration:  237 gradient_magnitude:  0.08471549563893352\n",
      "Iteration:  238 gradient_magnitude:  0.08471813918558789\n",
      "Iteration:  239 gradient_magnitude:  0.08472076824538112\n",
      "Iteration:  240 gradient_magnitude:  0.08472338285750204\n",
      "Iteration:  241 gradient_magnitude:  0.08472598306177015\n",
      "Iteration:  242 gradient_magnitude:  0.08472856889861874\n",
      "Iteration:  243 gradient_magnitude:  0.08473114040907805\n",
      "Iteration:  244 gradient_magnitude:  0.08473369763475887\n",
      "Iteration:  245 gradient_magnitude:  0.08473624061783638\n",
      "Iteration:  246 gradient_magnitude:  0.08473876940103421\n",
      "Iteration:  247 gradient_magnitude:  0.08474128402760883\n",
      "Iteration:  248 gradient_magnitude:  0.08474378454133423\n",
      "Iteration:  249 gradient_magnitude:  0.08474627098648678\n",
      "Iteration:  250 gradient_magnitude:  0.08474874340783035\n",
      "Iteration:  251 gradient_magnitude:  0.08475120185060186\n",
      "Iteration:  252 gradient_magnitude:  0.08475364636049684\n",
      "Iteration:  253 gradient_magnitude:  0.08475607698365537\n",
      "Iteration:  254 gradient_magnitude:  0.08475849376664825\n",
      "Iteration:  255 gradient_magnitude:  0.08476089675646342\n",
      "Iteration:  256 gradient_magnitude:  0.08476328600049259\n",
      "Iteration:  257 gradient_magnitude:  0.0847656615465181\n",
      "Iteration:  258 gradient_magnitude:  0.08476802344270006\n",
      "Iteration:  259 gradient_magnitude:  0.08477037173756362\n",
      "Iteration:  260 gradient_magnitude:  0.08477270647998657\n",
      "Iteration:  261 gradient_magnitude:  0.08477502771918721\n",
      "Iteration:  262 gradient_magnitude:  0.08477733550471211\n",
      "Iteration:  263 gradient_magnitude:  0.08477962988642458\n",
      "Iteration:  264 gradient_magnitude:  0.08478191091449286\n",
      "Iteration:  265 gradient_magnitude:  0.08478417863937897\n",
      "Iteration:  266 gradient_magnitude:  0.08478643311182732\n",
      "Iteration:  267 gradient_magnitude:  0.08478867438285394\n",
      "Iteration:  268 gradient_magnitude:  0.08479090250373555\n",
      "Iteration:  269 gradient_magnitude:  0.08479311752599916\n",
      "Iteration:  270 gradient_magnitude:  0.08479531950141153\n",
      "Iteration:  271 gradient_magnitude:  0.08479750848196906\n",
      "Iteration:  272 gradient_magnitude:  0.08479968451988772\n",
      "Iteration:  273 gradient_magnitude:  0.08480184766759341\n",
      "Iteration:  274 gradient_magnitude:  0.08480399797771218\n",
      "Iteration:  275 gradient_magnitude:  0.0848061355030607\n",
      "Iteration:  276 gradient_magnitude:  0.08480826029663718\n",
      "Iteration:  277 gradient_magnitude:  0.0848103724116121\n",
      "Iteration:  278 gradient_magnitude:  0.08481247190131938\n",
      "Iteration:  279 gradient_magnitude:  0.08481455881924758\n",
      "Iteration:  280 gradient_magnitude:  0.08481663321903123\n",
      "Iteration:  281 gradient_magnitude:  0.08481869515444247\n",
      "Iteration:  282 gradient_magnitude:  0.08482074467938278\n",
      "Iteration:  283 gradient_magnitude:  0.0848227818478748\n",
      "Iteration:  284 gradient_magnitude:  0.0848248067140544\n",
      "Iteration:  285 gradient_magnitude:  0.08482681933216292\n",
      "Iteration:  286 gradient_magnitude:  0.08482881975653934\n",
      "Iteration:  287 gradient_magnitude:  0.08483080804161304\n",
      "Iteration:  288 gradient_magnitude:  0.08483278424189612\n",
      "Iteration:  289 gradient_magnitude:  0.0848347484119765\n",
      "Iteration:  290 gradient_magnitude:  0.08483670060651055\n",
      "Iteration:  291 gradient_magnitude:  0.08483864088021639\n",
      "Iteration:  292 gradient_magnitude:  0.08484056928786694\n",
      "Iteration:  293 gradient_magnitude:  0.08484248588428332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  294 gradient_magnitude:  0.08484439072432824\n",
      "Iteration:  295 gradient_magnitude:  0.08484628386289979\n",
      "Iteration:  296 gradient_magnitude:  0.08484816535492491\n",
      "Iteration:  297 gradient_magnitude:  0.08485003525535346\n",
      "Iteration:  298 gradient_magnitude:  0.08485189361915209\n",
      "Iteration:  299 gradient_magnitude:  0.08485374050129836\n",
      "Iteration:  300 gradient_magnitude:  0.08485557595677504\n",
      "Iteration:  301 gradient_magnitude:  0.08485740004056429\n",
      "Iteration:  302 gradient_magnitude:  0.08485921280764236\n",
      "Iteration:  303 gradient_magnitude:  0.08486101431297394\n",
      "Iteration:  304 gradient_magnitude:  0.08486280461150698\n",
      "Iteration:  305 gradient_magnitude:  0.0848645837581675\n",
      "Iteration:  306 gradient_magnitude:  0.08486635180785443\n",
      "Iteration:  307 gradient_magnitude:  0.0848681088154347\n",
      "Iteration:  308 gradient_magnitude:  0.08486985483573833\n",
      "Iteration:  309 gradient_magnitude:  0.0848715899235537\n",
      "Iteration:  310 gradient_magnitude:  0.08487331413362287\n",
      "Iteration:  311 gradient_magnitude:  0.08487502752063698\n",
      "Iteration:  312 gradient_magnitude:  0.08487673013923193\n",
      "Iteration:  313 gradient_magnitude:  0.08487842204398378\n",
      "Iteration:  314 gradient_magnitude:  0.08488010328940476\n",
      "Iteration:  315 gradient_magnitude:  0.08488177392993893\n",
      "Iteration:  316 gradient_magnitude:  0.08488343401995808\n",
      "Iteration:  317 gradient_magnitude:  0.08488508361375788\n",
      "Iteration:  318 gradient_magnitude:  0.08488672276555394\n",
      "Iteration:  319 gradient_magnitude:  0.0848883515294779\n",
      "Iteration:  320 gradient_magnitude:  0.0848899699595739\n",
      "Iteration:  321 gradient_magnitude:  0.08489157810979474\n",
      "Iteration:  322 gradient_magnitude:  0.08489317603399858\n",
      "Iteration:  323 gradient_magnitude:  0.08489476378594529\n",
      "Iteration:  324 gradient_magnitude:  0.08489634141929307\n",
      "Iteration:  325 gradient_magnitude:  0.08489790898759532\n",
      "Iteration:  326 gradient_magnitude:  0.08489946654429724\n",
      "Iteration:  327 gradient_magnitude:  0.08490101414273284\n",
      "Iteration:  328 gradient_magnitude:  0.08490255183612173\n",
      "Iteration:  329 gradient_magnitude:  0.08490407967756625\n",
      "Iteration:  330 gradient_magnitude:  0.08490559772004849\n",
      "Iteration:  331 gradient_magnitude:  0.08490710601642754\n",
      "Iteration:  332 gradient_magnitude:  0.08490860461943656\n",
      "Iteration:  333 gradient_magnitude:  0.08491009358168025\n",
      "Iteration:  334 gradient_magnitude:  0.08491157295563215\n",
      "Iteration:  335 gradient_magnitude:  0.08491304279363199\n",
      "Iteration:  336 gradient_magnitude:  0.08491450314788344\n",
      "Iteration:  337 gradient_magnitude:  0.0849159540704514\n",
      "Iteration:  338 gradient_magnitude:  0.08491739561325982\n",
      "Iteration:  339 gradient_magnitude:  0.08491882782808939\n",
      "Iteration:  340 gradient_magnitude:  0.08492025076657521\n",
      "Iteration:  341 gradient_magnitude:  0.08492166448020468\n",
      "Iteration:  342 gradient_magnitude:  0.08492306902031538\n",
      "Iteration:  343 gradient_magnitude:  0.08492446443809304\n",
      "Iteration:  344 gradient_magnitude:  0.08492585078456949\n",
      "Iteration:  345 gradient_magnitude:  0.08492722811062078\n",
      "Iteration:  346 gradient_magnitude:  0.0849285964669652\n",
      "Iteration:  347 gradient_magnitude:  0.08492995590416161\n",
      "Iteration:  348 gradient_magnitude:  0.08493130647260753\n",
      "Iteration:  349 gradient_magnitude:  0.08493264822253745\n",
      "Iteration:  350 gradient_magnitude:  0.08493398120402125\n",
      "Iteration:  351 gradient_magnitude:  0.08493530546696243\n",
      "Iteration:  352 gradient_magnitude:  0.08493662106109671\n",
      "Iteration:  353 gradient_magnitude:  0.08493792803599043\n",
      "Iteration:  354 gradient_magnitude:  0.08493922644103907\n",
      "Iteration:  355 gradient_magnitude:  0.08494051632546588\n",
      "Iteration:  356 gradient_magnitude:  0.08494179773832047\n",
      "Iteration:  357 gradient_magnitude:  0.08494307072847751\n",
      "Iteration:  358 gradient_magnitude:  0.08494433534463548\n",
      "Iteration:  359 gradient_magnitude:  0.08494559163531537\n",
      "Iteration:  360 gradient_magnitude:  0.08494683964885956\n",
      "Iteration:  361 gradient_magnitude:  0.0849480794334306\n",
      "Iteration:  362 gradient_magnitude:  0.08494931103701019\n",
      "Iteration:  363 gradient_magnitude:  0.08495053450739802\n",
      "Iteration:  364 gradient_magnitude:  0.08495174989221083\n",
      "Iteration:  365 gradient_magnitude:  0.08495295723888147\n",
      "Iteration:  366 gradient_magnitude:  0.08495415659465774\n",
      "Iteration:  367 gradient_magnitude:  0.08495534800660177\n",
      "Iteration:  368 gradient_magnitude:  0.08495653152158897\n",
      "Iteration:  369 gradient_magnitude:  0.08495770718630723\n",
      "Iteration:  370 gradient_magnitude:  0.08495887504725619\n",
      "Iteration:  371 gradient_magnitude:  0.0849600351507464\n",
      "Iteration:  372 gradient_magnitude:  0.08496118754289869\n",
      "Iteration:  373 gradient_magnitude:  0.0849623322696434\n",
      "Iteration:  374 gradient_magnitude:  0.08496346937671978\n",
      "Iteration:  375 gradient_magnitude:  0.08496459890967532\n",
      "Iteration:  376 gradient_magnitude:  0.08496572091386527\n",
      "Iteration:  377 gradient_magnitude:  0.08496683543445194\n",
      "Iteration:  378 gradient_magnitude:  0.08496794251640424\n",
      "Iteration:  379 gradient_magnitude:  0.08496904220449725\n",
      "Iteration:  380 gradient_magnitude:  0.08497013454331168\n",
      "Iteration:  381 gradient_magnitude:  0.08497121957723341\n",
      "Iteration:  382 gradient_magnitude:  0.0849722973504532\n",
      "Iteration:  383 gradient_magnitude:  0.08497336790696623\n",
      "Iteration:  384 gradient_magnitude:  0.0849744312905717\n",
      "Iteration:  385 gradient_magnitude:  0.08497548754487266\n",
      "Iteration:  386 gradient_magnitude:  0.0849765367132756\n",
      "Iteration:  387 gradient_magnitude:  0.0849775788389902\n",
      "Iteration:  388 gradient_magnitude:  0.08497861396502908\n",
      "Iteration:  389 gradient_magnitude:  0.08497964213420768\n",
      "Iteration:  390 gradient_magnitude:  0.08498066338914392\n",
      "Iteration:  391 gradient_magnitude:  0.08498167777225812\n",
      "Iteration:  392 gradient_magnitude:  0.08498268532577287\n",
      "Iteration:  393 gradient_magnitude:  0.0849836860917128\n",
      "Iteration:  394 gradient_magnitude:  0.08498468011190466\n",
      "Iteration:  395 gradient_magnitude:  0.08498566742797702\n",
      "Iteration:  396 gradient_magnitude:  0.08498664808136042\n",
      "Iteration:  397 gradient_magnitude:  0.08498762211328721\n",
      "Iteration:  398 gradient_magnitude:  0.08498858956479162\n",
      "Iteration:  399 gradient_magnitude:  0.08498955047670968\n",
      "Iteration:  400 gradient_magnitude:  0.08499050488967931\n",
      "Iteration:  401 gradient_magnitude:  0.08499145284414036\n",
      "Iteration:  402 gradient_magnitude:  0.0849923943803347\n",
      "Iteration:  403 gradient_magnitude:  0.08499332953830614\n",
      "Iteration:  404 gradient_magnitude:  0.08499425835790085\n",
      "Iteration:  405 gradient_magnitude:  0.08499518087876715\n",
      "Iteration:  406 gradient_magnitude:  0.08499609714035587\n",
      "Iteration:  407 gradient_magnitude:  0.08499700718192046\n",
      "Iteration:  408 gradient_magnitude:  0.08499791104251705\n",
      "Iteration:  409 gradient_magnitude:  0.08499880876100478\n",
      "Iteration:  410 gradient_magnitude:  0.084999700376046\n",
      "Iteration:  411 gradient_magnitude:  0.08500058592610632\n",
      "Here are the final weights after convergence:\n",
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#STUDENT: Specify the initial_weights, step_size, and tolerance\n",
    "simple_feature_matrix = B\n",
    "output = train_labels\n",
    "\n",
    "\n",
    "\n",
    "temp_weights = [np.ones(len(train_data[0]))]*(len(train_data[0]))# this makes all the predictions 0 #np.array([0.]*2*len(simple_feature_matrix))\n",
    "temp_weights=np.array(temp_weights)\n",
    "initial_weights = temp_weights.reshape(len(simple_feature_matrix),len(train_data[0]))\n",
    "\n",
    "step_size = 1\n",
    "tolerance = 0.085\n",
    "# end of code\n",
    "\n",
    "# Use the regression_gradient_descent function to calculate the gradient decent and store it in the variable 'final_weights'\n",
    "final_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)\n",
    "\n",
    "# end of code\n",
    "print (\"Here are the final weights after convergence:\")\n",
    "print (final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15705\n",
      "71\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (71,71) (71,15705) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-5423b121aa4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_weights\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (71,71) (71,15705) "
     ]
    }
   ],
   "source": [
    "print(len(B[0]))\n",
    "print(len(final_weights[0]))\n",
    "\n",
    "print(np.dot(np.transpose(final_weights*B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
