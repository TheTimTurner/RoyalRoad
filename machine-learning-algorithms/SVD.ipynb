{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import random\n",
    "#!pip install ProgressBar\n",
    "from progressbar import ProgressBar\n",
    "import math\n",
    "#!{sys.executable} -m pip install ProgressBar\n",
    "#!pip install ProgressBar\n",
    "from random import randrange\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "#svd stuff\n",
    "from scipy.linalg import svd\n",
    "from numpy.linalg import pinv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the titles and wheater the book is alive or dead into two seperate arrays\n",
    "def loadResult():\n",
    "    test_labels = []\n",
    "    test_values = []\n",
    "    with open('table-of-contents.json', encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "        for books in data:\n",
    "            for book in data[books]:\n",
    "                test_labels.append(book['title'])\n",
    "                test_values.append(book['dead'])\n",
    "                \n",
    "    return test_labels, test_values\n",
    "\n",
    "#get the data from the data output file\n",
    "def loadData():\n",
    "    final_data = []\n",
    "    with open('dataOutput2.json', encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "        for index in data: \n",
    "            final_data.append(data[index]['chapData'])\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16705\n",
      "Training dataset dimensions:  (15705, 71)\n",
      "Number of training labels:  15705\n",
      "Testing dataset dimensions:  (1000, 71)\n",
      "Number of testing labels:  1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                \n",
    "\n",
    "title_labels, dead_labels = loadResult()\n",
    "train = loadData()\n",
    "print(len(train))\n",
    "## Load the training set\n",
    "train_data= np.array(train[:15705])\n",
    "train_labels = np.array(dead_labels[:15705])\n",
    "train_titles = np.array(title_labels[:15705])\n",
    "\n",
    "\n",
    "## Load the testing set\n",
    "\n",
    "test_data= np.array(train[-1000:])\n",
    "test_labels = np.array(dead_labels[-1000:])\n",
    "test_titles = np.array(title_labels[-1000:])\n",
    "\n",
    "\n",
    "## Print out their dimensions\n",
    "print(\"Training dataset dimensions: \", np.shape(train_data))\n",
    "print(\"Number of training labels: \", len(train_labels))\n",
    "print(\"Testing dataset dimensions: \", np.shape(test_data))\n",
    "print(\"Number of testing labels: \", len(test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next two items we are attempting to recreating the element. A should equal B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate pseudo inverse manually and compare to the built in version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   1  36]\n",
      " [  0   0   0 ...   0   1   7]\n",
      " [  0   0   0 ...  20  25 689]\n",
      " ...\n",
      " [  0   0   0 ...   1   0   5]\n",
      " [  0   0   0 ...   2   0   1]\n",
      " [  0   0   1 ... 187 255 232]]\n",
      "[[ 1.41665154e-06 -7.87624385e-05 -1.50063288e-05 ... -8.81508881e-05\n",
      "  -2.43381414e-04 -1.57186015e-04]\n",
      " [-3.11887480e-05  4.14462675e-05 -2.39004888e-04 ... -1.38122724e-04\n",
      "  -1.99278900e-05 -3.38553861e-04]\n",
      " [-2.30330965e-05 -3.87923977e-05 -8.42779723e-05 ... -1.93302046e-04\n",
      "  -1.29650184e-05  1.44900618e-03]\n",
      " ...\n",
      " [-3.53724560e-07  4.86100109e-07  1.05452792e-06 ...  6.99338964e-07\n",
      "   5.91041688e-07 -3.66002797e-06]\n",
      " [ 4.49952314e-07 -4.67608076e-07 -1.13338360e-07 ... -6.02837625e-07\n",
      "  -5.32801834e-07  3.21050947e-06]\n",
      " [-3.71539162e-09 -5.57172465e-08  5.07890139e-07 ... -1.54623509e-07\n",
      "  -2.89767954e-08 -2.49413822e-07]]\n"
     ]
    }
   ],
   "source": [
    "#A= np.array([[0.1, 0.2],[0.3, 0.4],[0.5, 0.6],[0.7, 0.8]])\n",
    "A=np.array(train_data[:])\n",
    "print(A)\n",
    "# calculate pseudoinverse\n",
    "B = pinv(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   1  36]\n",
      " [  0   0   0 ...   0   1   7]\n",
      " [  0   0   0 ...  20  25 689]\n",
      " ...\n",
      " [  0   0   0 ...   1   0   5]\n",
      " [  0   0   0 ...   2   0   1]\n",
      " [  0   0   1 ... 187 255 232]]\n",
      "[[ 1.41665154e-06 -7.87624385e-05 -1.50063288e-05 ... -8.81508881e-05\n",
      "  -2.43381414e-04 -1.57186015e-04]\n",
      " [-3.11883520e-05  4.14462206e-05 -2.39004961e-04 ... -1.38122710e-04\n",
      "  -1.99278888e-05 -3.38553853e-04]\n",
      " [-2.30322454e-05 -3.87925141e-05 -8.42781799e-05 ... -1.93302006e-04\n",
      "  -1.29647602e-05  1.44900626e-03]\n",
      " ...\n",
      " [-3.53724719e-07  4.86100125e-07  1.05452794e-06 ...  6.99338961e-07\n",
      "   5.91041740e-07 -3.66002796e-06]\n",
      " [ 4.49952818e-07 -4.67608132e-07 -1.13338439e-07 ... -6.02837610e-07\n",
      "  -5.32801893e-07  3.21050947e-06]\n",
      " [-3.71532570e-09 -5.57172542e-08  5.07890127e-07 ... -1.54623507e-07\n",
      "  -2.89767968e-08 -2.49413821e-07]]\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "# calculate svd\n",
    "U, s, VT = svd(A)\n",
    "# reciprocals of s\n",
    "d = 1.0 / s\n",
    "# create m x n D matrix\n",
    "D = np.zeros(A.shape)\n",
    "# populate D with n x n diagonal matrix\n",
    "D[:A.shape[1], :A.shape[1]] = np.diag(d)\n",
    "# calculate pseudoinverse\n",
    "B = VT.T.dot(D.T).dot(U.T)\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getError(B):\n",
    "\n",
    "    true = 0\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        x = train_data[i]\n",
    "        predict = np.sum(x.dot(B))\n",
    "\n",
    "        #find the error\n",
    "        if (train_labels[i] == False) and (predict<1):\n",
    "            true +=1\n",
    "        elif (train_labels[i] == True) and (predict>1):\n",
    "            true+=1\n",
    "\n",
    "\n",
    "\n",
    "        #these are  the results from the svd classification\n",
    "        #print(predict, end = \": \")\n",
    "        #print(train_labels[i])\n",
    "\n",
    "        #calculate the error\n",
    "\n",
    "    #print('error rate: ', 1-(true/len(train_data)))\n",
    "    return 1-(true/len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate:  0.3444126074498567\n"
     ]
    }
   ],
   "source": [
    "print('error rate: ', getError(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compute the derivative of the regression cost function: \n",
    "$$L_D(w) = \\frac{1}{n}\\sum_{i=1}^n (y_i-w\\cdot x_i)^2,$$\n",
    "where $x_i\\in \\mathrm{R}^d$ is the input feature of dimension $d$, $y_i\\in\\mathrm{R}$ is the output response, and $w\\in\\mathrm{R}^d$ is the regression weights.\n",
    "\n",
    "**Task P3:** Complete the function 'weight_derivative' to calculate the derivative of the cost function with respect to regression weights $w$, i.e., $\\frac{\\partial}{\\partial w}L_D(w)$. Note that this should be a $d$ dimensional vector. Also copy the output of the code for the test example to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_derivative(weights, feature_matrix, labels):\n",
    "    # Input:\n",
    "    # weights: weight vector w, a numpy vector of dimension d\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d\n",
    "    # Output:\n",
    "    # Derivative of the regression cost function with respect to the weight w, a numpy array of dimension d\n",
    "        \n",
    "    ## STUDENT: Start of code ###\n",
    "    \n",
    "    sum = 0\n",
    "    #get array of normalized dot produts\n",
    "    dot_product=weights.dot(feature_matrix)\n",
    "    #sum the diffrence between dot and true value\n",
    "    for i in range(len(feature_matrix)):\n",
    "        \n",
    "        diffrence = labels[i] - dot_product[i]\n",
    "        sum = sum + diffrence\n",
    "    #finsh derive ative of regressive cost function    \n",
    "    return sum/(len(feature_matrix))*2\n",
    "    # End of code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6901408450704225\n"
     ]
    }
   ],
   "source": [
    "# NOTE: copy the output to the solution file.\n",
    "\n",
    "\n",
    "\n",
    "my_weights = np.zeros(len(train_data[0])) # this makes all the predictions 0\n",
    "derivative = weight_derivative(my_weights, B, train_labels)\n",
    "\n",
    "print (derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will write a function to perform gradient descent algorithm on the lineare regression cost. Given an initial point, we will update the current weights by moving in the negative gradient direction to minimize the cost function. Thus, in each iteration we obtain the updated weight $w_{t+1}$ from the current iterate $w_t$ as follows:\n",
    "$$w_{t+1} = w_t - h\\frac{\\partial}{\\partial w}L_D(w_t),$$\n",
    "where $h$ is the 'step_size' that is the amount by which we move in the negative gradient direction. \n",
    "\n",
    "We stop when we are sufficiently close to the optimum (where gradient is the zero vector) by checking the condition with respect to the magnitude (length) of the gradient vector:\n",
    "$$\\|\\frac{\\partial}{\\partial w}L_D(w_t)\\|_2\\leq \\epsilon,$$\n",
    "where $\\epsilon$ is the 'tolerance' parameter.\n",
    "\n",
    "**Task P4:** Complete the code section to perform the gradient decent in the function `regression_gradient_descent`. Copy the code to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, labels, initial_weights, step_size, tolerance):\n",
    "    # Gradient descent algorithm for linear regression problem    \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d\n",
    "    # initial_weights: initial weight vector to start with, a numpy vector of dimension d\n",
    "    # step_size: step size of update\n",
    "    # tolerance: tolerace epsilon for stopping condition\n",
    "    # Output:\n",
    "    # Weights obtained after convergence\n",
    "    \n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # current iterate\n",
    "    i = 0\n",
    "    while not converged:\n",
    "        i += 1\n",
    "        # STUDENT: Start of code: your impelementation of what the gradient descent algorithm does in every iteration\n",
    "        # Refer back to the update rule listed above: update the weight\n",
    "        # Compute the gradient magnitude:\n",
    "        \n",
    "        weight_deriv = weight_derivative(weights[i-1], feature_matrix,labels)\n",
    "        \n",
    "        temp_gradient = [0]*len(weights[i-1])\n",
    "        \n",
    "        for a in range(len(weights[i-1])):\n",
    "            weight_deriv= weight_deriv*step_size\n",
    "            temp_gradient[a] =  weights[i-1][a] - weight_deriv\n",
    "        gradient_magnitude = temp_gradient\n",
    "        \n",
    "        #size of gradient magnitude\n",
    "        gradient_size = 0\n",
    "        \n",
    "        for a in gradient_magnitude:\n",
    "            gradient_size += pow(a,2)\n",
    "        gradient_size = pow(gradient_size,1/2)\n",
    "        \n",
    "        #reasign weights\n",
    "        for j in range(len(weights[i])):\n",
    "            weights[i][j]=gradient_magnitude[j]\n",
    "            #weights[i][j]=temp_gradient[j]\n",
    "            \n",
    "        # Check the stopping condition to decide whether you want to stop the iterations\n",
    "        if (gradient_size>=tolerance):             # STUDENT: check the stopping condition here\n",
    "            converged = True\n",
    "        if (i>=len(weights)-1):             # STUDENT: check the stopping condition here\n",
    "            converged = True\n",
    "        # End of code\n",
    "        \n",
    "        print (\"Iteration: \",i,\"gradient_magnitude: \", np.sum(gradient_magnitude)) # for us to check about convergence\n",
    "        print('error rate: ', getError(gradient_magnitude))\n",
    "        \n",
    "    return gradient_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 71 into shape (71,71)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-94b95a82f735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtemp_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m# this makes all the predictions 0 #np.array([0.]*2*len(simple_feature_matrix))\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtemp_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0minitial_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimple_feature_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 71 into shape (71,71)"
     ]
    }
   ],
   "source": [
    "#STUDENT: Specify the initial_weights, step_size, and tolerance\n",
    "simple_feature_matrix = B\n",
    "output = train_labels\n",
    "\n",
    "\n",
    "\n",
    "temp_weights = [np.zeros(len(train_data[0]))]# this makes all the predictions 0 #np.array([0.]*2*len(simple_feature_matrix))\n",
    "temp_weights=np.array(temp_weights)\n",
    "initial_weights = temp_weights.reshape(len(simple_feature_matrix),len(train_data[0]))\n",
    "\n",
    "step_size = 0.05\n",
    "tolerance = 10\n",
    "# end of code\n",
    "\n",
    "# Use the regression_gradient_descent function to calculate the gradient decent and store it in the variable 'final_weights'\n",
    "final_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)\n",
    "\n",
    "# end of code\n",
    "print (\"Here are the final weights after convergence:\")\n",
    "print (final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
